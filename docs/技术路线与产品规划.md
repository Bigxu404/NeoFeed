# NeoFeed 技术路线与产品规划

> **核心理念**：Don't collect information. Cultivate understanding.
> 
> **一句话定位**：自动生成你每周思想结晶的 AI 助手

---

## 一、产品功能概述

### 核心功能

**NeoFeed 做三件事：**

1. **无摩擦输入** - 从多渠道收集信息（微信、网页、Telegram、GPT对话）
2. **智能处理** - AI 自动摘要、分类、提取关键词
3. **周期输出** - 每周自动生成知识周报，提炼洞察

### 产品价值

| 痛点 | 解决方案 |
|------|----------|
| 信息分散在微信/网页/笔记 | 统一入口，一键收藏 |
| 收集后不整理，堆积如山 | AI 自动分类和摘要 |
| 从不回顾，信息变垃圾 | 每周自动生成报告 |
| 缺乏洞察，没有产出 | AI 聚类分析，提炼主题 |

### 目标用户

- 🧑‍💻 信息型工作者（产品经理、设计师、分析师）
- 🧠 深度思考者（喜欢阅读和学习）
- 🤖 AI 重度用户（经常用 GPT/Claude）
- 📱 碎片收集者（日常从多渠道获取信息）

---

## 二、产品功能拆解

### V1.0 - 个人提效 MVP（核心功能）

**目标**：帮助自己高效收集和总结信息

#### 功能模块

```
1. 信息收集
   ├─ Telegram Bot（转发消息/链接）
   ├─ 网页解析（自动抓取正文）
   └─ 手动输入（文本/笔记）

2. AI 自动处理
   ├─ 生成摘要（100-300字）
   ├─ 主题分类（AI趋势/产品思考/技术分享等）
   ├─ 关键词提取（5-8个关键词）
   └─ 重要性评分（0-1分）

3. 数据存储
   ├─ 原始信息
   ├─ AI 处理结果
   └─ 结构化索引

4. 周报生成
   ├─ 时间范围统计（本周收集了多少条）
   ├─ 主题聚类（AI 自动分组相似主题）
   ├─ 关键词统计（高频词汇）
   ├─ 核心洞察（GPT 生成总结）
   └─ Markdown 输出

5. 简单查看
   ├─ 列表展示所有条目
   ├─ 按分类筛选
   └─ 查看周报历史
```

### V2.0 - 智能回顾与分析（未来）

```
1. 语义搜索
   - "找我之前收藏的关于产品增长的内容"
   - 基于向量相似度

2. 时间线回顾
   - 过去3个月我关注了什么主题
   - 兴趣变化趋势分析

3. 智能提醒
   - 基于时间推荐复盘内容
   - "你已经3周没看这个主题了"

4. 趋势分析
   - 我的关注点在如何演化
   - 热度曲线可视化
```

### V3.0 - 知识网络（远期）

```
1. 知识图谱
   - 可视化思维网络
   - 概念之间的关联

2. 个性化洞察
   - 模仿用户的写作风格
   - 生成观点报告

3. 多模态支持
   - 图片识别
   - 音频转文字

4. 协作功能
   - 团队知识共享
   - 协作周报
```

---

## 三、技术架构

### 系统架构图

```
┌─────────────────────────────────────────┐
│         输入层 (Input Layer)            │
│  Telegram Bot / 网页 / Chrome插件       │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│       采集层 (Collection Layer)         │
│  内容抓取 / 格式标准化 / 去重            │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│      AI 处理层 (AI Processing)          │
│  OpenAI API / 摘要 / 分类 / 向量化      │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│        存储层 (Storage Layer)           │
│  SQLite → PostgreSQL + pgvector         │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│      分析层 (Analytics Layer)           │
│  聚类分析 / 趋势识别 / 周报生成          │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│        输出层 (Output Layer)            │
│  Web Dashboard / Markdown / 邮件推送    │
└─────────────────────────────────────────┘
```

### 技术栈选择

#### 前端
- **框架**：Next.js 14 (App Router)
- **语言**：TypeScript
- **样式**：Tailwind CSS
- **组件**：shadcn/ui
- **部署**：Vercel

#### 后端
- **API**：FastAPI (Python) 或 Next.js API Routes
- **任务队列**：Redis + Celery (异步AI处理)
- **定时任务**：APScheduler (每周生成报告)

#### 数据库
- **本地开发**：SQLite
- **生产环境**：Supabase (PostgreSQL + pgvector)
- **向量搜索**：pgvector (语义搜索)

#### AI 服务
- **LLM**：OpenAI GPT-4o-mini (成本优化)
- **Embedding**：text-embedding-3-small (向量化)
- **备选**：Claude 3.5 Sonnet (长文本处理)

#### 输入端
- **Telegram Bot**：python-telegram-bot
- **网页抓取**：Jina Reader API / newspaper3k
- **Chrome 插件**：Manifest V3

---

## 四、技术路线图

### 阶段 0：基础设施搭建（Week 1-2）

**目标**：把基础打牢

```
✅ 数据库设计与初始化
   - 已完成：SQLite schema
   - 已完成：测试数据
   - 待做：Supabase 项目创建

✅ 开发环境配置
   - Python 3.10+
   - Node.js 18+
   - Git 版本管理

□ API 密钥准备
   - OpenAI API Key
   - Telegram Bot Token
   - Supabase 连接信息
```

### 阶段 1：MVP 核心功能（Week 3-6）

**目标**：实现完整的数据流（收集→处理→输出）

#### Week 3-4：输入与存储
```
□ Telegram Bot 开发
   ├─ 接收文本消息
   ├─ 接收链接（网页URL）
   ├─ 存储到数据库
   └─ 返回确认信息

□ 网页内容抓取
   ├─ 集成 Jina Reader API
   ├─ 提取标题、正文
   └─ 清洗HTML标签

□ 数据库连接层
   ├─ 封装增删改查
   ├─ 事务处理
   └─ 错误处理
```

#### Week 5：AI 处理模块
```
□ OpenAI 集成
   ├─ API 封装
   ├─ 错误重试机制
   └─ 成本监控

□ 自动摘要
   ├─ Prompt 设计（3句话摘要）
   ├─ 批量处理优化
   └─ 结果存储

□ 分类与标签
   ├─ 预设10-15个分类
   ├─ 关键词提取（5-8个）
   └─ 重要性评分（0-1）

□ 异步任务队列
   ├─ Celery 配置
   ├─ Redis 部署
   └─ 任务状态追踪
```

#### Week 6：周报生成
```
□ 数据聚合
   ├─ 时间范围查询（过去7天）
   ├─ 按分类统计
   └─ 关键词频次统计

□ 主题聚类
   ├─ 基于关键词相似度
   ├─ 简单版本（关键词匹配）
   └─ 高级版本（向量聚类）

□ 报告生成
   ├─ Markdown 模板设计
   ├─ GPT 生成洞察总结
   └─ 保存到数据库

□ 定时任务
   ├─ 每周日早上9点自动执行
   ├─ 生成并发送周报
   └─ 推送到 Telegram
```

### 阶段 2：Web 界面（Week 7-8）

**目标**：提供可视化查看界面

```
□ Next.js 项目搭建
   ├─ 项目初始化
   ├─ Tailwind CSS 配置
   └─ shadcn/ui 组件安装

□ 核心页面开发
   ├─ 首页：Dashboard（统计概览）
   ├─ 列表页：所有信息条目
   ├─ 详情页：单条信息展示
   └─ 周报页：周报列表和详情

□ 基础功能
   ├─ 搜索（标题/关键词）
   ├─ 筛选（按分类/来源）
   ├─ 排序（时间/重要性）
   └─ 手动输入（Web表单）

□ API 对接
   ├─ 获取信息列表
   ├─ 获取单条详情
   ├─ 获取周报列表
   └─ 提交新信息
```

### 阶段 3：优化与完善（Week 9-10）

**目标**：提升体验和稳定性

```
□ 性能优化
   ├─ 数据库索引优化
   ├─ API 响应速度
   ├─ 前端加载优化
   └─ 图片懒加载

□ 错误处理
   ├─ AI 调用失败重试
   ├─ 网页抓取失败降级
   ├─ 用户友好的错误提示
   └─ 日志记录

□ 成本控制
   ├─ 使用 GPT-4o-mini
   ├─ 批量处理减少调用
   ├─ 缓存相似内容摘要
   └─ 监控每日花费

□ 用户体验
   ├─ Loading 状态
   ├─ 空状态提示
   ├─ 操作反馈
   └─ 响应式设计
```

### 阶段 4：部署上线（Week 11-12）

**目标**：从本地迁移到生产环境

```
□ 数据库迁移
   ├─ SQLite → Supabase
   ├─ 数据验证
   └─ 连接配置更新

□ 服务部署
   ├─ 前端：Vercel
   ├─ 后端：Railway / Render
   ├─ Redis：Upstash
   └─ 环境变量配置

□ 域名与SSL
   ├─ 域名购买（可选）
   ├─ DNS 配置
   └─ HTTPS 证书

□ 监控与日志
   ├─ Sentry（错误追踪）
   ├─ PostHog（使用分析）
   └─ 日志聚合

□ 自己使用2-4周
   ├─ 收集真实数据
   ├─ 发现问题并修复
   └─ 验证产品价值
```

---

## 五、数据库设计

### 核心表结构

```
users (用户)
├─ id
├─ email
├─ telegram_id
└─ preferences (JSON)

items (信息条目)
├─ id
├─ user_id
├─ title
├─ content
├─ url
├─ source_type (telegram/wechat/web/gpt)
├─ status (pending/processed/failed)
└─ created_at

ai_results (AI处理结果)
├─ id
├─ item_id (1对1)
├─ summary
├─ category
├─ keywords (数组)
├─ importance_score
└─ sentiment

weekly_reports (周报)
├─ id
├─ user_id
├─ week_start / week_end
├─ content (Markdown)
├─ stats (JSON)
├─ clusters (JSON)
└─ status (draft/published)

tags (标签)
item_tags (条目-标签关联)
report_items (周报-条目关联)
processing_logs (处理日志)
```

### 数据流转

```
1. 用户转发消息 → items (status: pending)
2. Celery 任务触发 → AI 处理
3. 保存结果 → ai_results
4. 更新状态 → items (status: processed)
5. 每周日 → 聚合分析 → weekly_reports
```

---

## 六、核心模块实现细节

### Module 1：Telegram Bot

```python
# 功能点
1. 接收文本消息 → 存储原文
2. 接收链接 → 触发网页抓取 → 存储内容
3. 返回确认 → "✅ 已保存，ID: 123"
4. 提供命令
   /stats - 查看统计
   /report - 生成本周报告
   /list - 最近10条
```

### Module 2：AI 处理引擎

```python
# Prompt 设计示例
摘要 Prompt:
"""
请用3句话概括以下内容的核心要点：
{content}

要求：
1. 第一句话：整体概括
2. 第二句话：关键观点
3. 第三句话：启发或结论
"""

分类 Prompt:
"""
请将以下内容分类到这些主题之一：
AI趋势 / 产品思考 / 技术分享 / 设计 / 创业 / 其他

内容：{content}

只返回分类名称。
"""

关键词 Prompt:
"""
从以下内容中提取5-8个关键词：
{content}

用逗号分隔，只返回关键词。
"""
```

### Module 3：周报生成

```python
# 生成流程
1. 查询数据
   - 获取过去7天的所有 processed items
   - 关联 ai_results

2. 统计分析
   - 总条目数
   - 按分类统计
   - 按来源统计
   - 关键词频次（Counter）

3. 主题聚类（简单版）
   - 基于 category 分组
   - 每组提取代表性内容
   - GPT 生成该组的主题总结

4. 生成报告
   - 使用 Markdown 模板
   - 填充统计数据
   - GPT 生成"核心洞察"段落
   - 保存到 weekly_reports

5. 推送
   - Telegram 发送报告链接
   - 可选：邮件发送
```

---

## 七、成本估算

### 月度成本（个人使用）

| 项目 | 成本 | 说明 |
|------|------|------|
| OpenAI API | $10-30 | GPT-4o-mini，每天处理20条 |
| Supabase | 免费 | 500MB数据库 + 1GB存储 |
| Vercel | 免费 | 前端托管 |
| Railway | $5 | 后端API + Celery |
| Upstash Redis | 免费 | 10k命令/天 |
| 域名（可选） | $12/年 | - |
| **合计** | **$15-35/月** | 取决于使用量 |

### 成本优化策略

```
1. 使用 GPT-4o-mini 而非 GPT-4（便宜10倍）
2. 批量处理（一次API调用处理多条）
3. 缓存重复内容的摘要
4. 设置每日花费上限
5. 监控Token使用量
```

---

## 八、风险与应对

### 技术风险

| 风险 | 影响 | 应对方案 |
|------|------|----------|
| OpenAI API 不稳定 | 处理失败 | 重试机制 + 备选模型(Claude) |
| 网页抓取被封 | 内容获取失败 | 多个抓取服务轮换 |
| 数据库迁移出错 | 数据丢失 | 先备份 + 验证脚本 |
| AI 成本超预算 | 财务压力 | 每日限额 + 使用监控 |

### 产品风险

| 风险 | 影响 | 应对方案 |
|------|------|----------|
| 自己不用了 | 项目停滞 | 保持产品简单，降低维护成本 |
| 周报质量不高 | 价值感低 | 迭代优化Prompt，A/B测试 |
| 信息过载 | 周报太长 | 智能筛选重要内容 |
| 缺乏洞察 | 只是搬运 | GPT生成深度分析 |

---

## 九、关键决策记录

### 为什么选 SQLite → PostgreSQL 路线？

```
✅ 优势：
- 本地快速验证，无需网络
- 熟悉的 SQL 语法
- 迁移脚本已准备好
- 数据结构可平滑过渡

❌ 替代方案（为什么不选）：
- 直接用 Supabase：开发时需要网络，调试慢
- MongoDB：关系型数据更适合本场景
- Notion API：性能差，限制多
```

### 为什么用 FastAPI 而非纯 Next.js？

```
✅ 优势：
- Python 生态丰富（AI/数据处理）
- 异步任务队列（Celery）成熟
- 与 OpenAI SDK 集成更自然

⚠️ 折中方案：
- 简单功能可以用 Next.js API Routes
- 复杂 AI 处理用 FastAPI
- 根据实际情况灵活调整
```

### 为什么 V1 不做向量搜索？

```
✅ 原因：
- V1 重点是验证核心流程（收集→处理→周报）
- 向量搜索需要 Embedding（增加成本和复杂度）
- 关键词搜索已够用（SQLite FTS）
- 等到 V2 迁移到 PostgreSQL 时再加

V2 加入向量搜索：
- 使用 pgvector
- 语义搜索："找关于产品设计的内容"
- 智能推荐："你可能感兴趣的内容"
```

---

## 十、里程碑与验收标准

### Milestone 1：MVP 可用（Week 6）

**验收标准：**
- [ ] Telegram Bot 能接收消息并存储
- [ ] AI 能自动生成摘要和分类
- [ ] 能手动触发生成本周周报
- [ ] 周报包含：统计、聚类、洞察
- [ ] 所有数据存在 SQLite

**演示内容：**
给自己发10条消息 → 触发处理 → 生成周报

### Milestone 2：Web 可访问（Week 8）

**验收标准：**
- [ ] 能在浏览器查看所有信息
- [ ] 能搜索和筛选
- [ ] 能查看周报历史
- [ ] 界面美观、响应式

**演示内容：**
打开网页 → 浏览信息 → 查看周报

### Milestone 3：生产部署（Week 12）

**验收标准：**
- [ ] 数据已迁移到 Supabase
- [ ] 服务稳定运行在云端
- [ ] 自己使用2周无重大问题
- [ ] 成本在预算内（<$30/月）

**演示内容：**
展示真实使用的周报和数据

---

## 十一、后续扩展方向

### 短期优化（3-6个月）

```
1. Chrome 插件
   - 一键保存网页
   - 选中文字右键收藏

2. 微信集成
   - 公众号文章自动收集
   - 文件传输助手同步

3. 邮件推送
   - 每周报告发邮件
   - 可自定义格式

4. 标签系统增强
   - 自动打标签
   - 标签管理界面
```

### 中期功能（6-12个月）

```
1. 语义搜索（V2.0）
   - pgvector 向量搜索
   - "找我之前看过的关于AI的内容"

2. 知识图谱（V2.5）
   - 概念之间的关联
   - 可视化网络

3. 移动端适配
   - 响应式优化
   - PWA 支持

4. 协作功能
   - 团队共享
   - 评论讨论
```

### 长期愿景（1年+）

```
1. 平台化（V3.0）
   - 开放 API
   - 第三方集成
   - 插件生态

2. 智能化升级
   - 个性化推荐
   - 自动生成文章
   - 思维模式分析

3. 商业化探索
   - 个人版：免费
   - Pro 版：高级功能
   - 团队版：协作功能
```

---

## 十二、开发原则

### 产品原则

1. **简单至上** - 每个功能都要问：真的需要吗？
2. **输出导向** - 不只是收集，要有产出（周报）
3. **自动化优先** - 能自动的绝不手动
4. **自己先用** - 解决自己的问题

### 技术原则

1. **先完成，再完美** - MVP 优先，不过度设计
2. **渐进式迁移** - SQLite → PostgreSQL，平滑过渡
3. **成本可控** - 监控每一笔支出
4. **可维护性** - 代码清晰，文档完善

### 迭代原则

1. **两周一个版本** - 快速迭代，及时调整
2. **用数据说话** - 记录使用情况，优化决策
3. **保持专注** - 一次只做一件事
4. **及时止损** - 不work的功能果断删除

---

## 附录：参考资源

### 技术文档
- [OpenAI API 文档](https://platform.openai.com/docs)
- [Supabase 文档](https://supabase.com/docs)
- [Next.js 文档](https://nextjs.org/docs)
- [FastAPI 文档](https://fastapi.tiangolo.com/)
- [pgvector 文档](https://github.com/pgvector/pgvector)

### 开发工具
- [DB Browser for SQLite](https://sqlitebrowser.org/)
- [Postman](https://www.postman.com/) - API 测试
- [Cursor](https://cursor.sh/) - AI 辅助编程

### 灵感来源
- Readwise - 阅读同步
- Mem - 智能笔记
- Notion - 通用笔记
- Heptabase - 思维笔记

---

**文档版本**：v1.0  
**最后更新**：2025-11-10  
**维护者**：NeoFeed Team

---

*这个文档是活的，会随着产品开发不断更新和完善。*

